training:
  #batch size: [synth, weak, unlabel]
  batch_size: [24, 24, 48, 48]  # if you want to change the batch size, be careful of the total training steps.
  batch_size_val: 64
  const_max: 70 # max weight used for self supervised loss
  n_epochs_warmup: 10 # num epochs used for exponential warmup
  num_workers: 1 # change according to your cpu
  n_epochs: 50 # 250 max num epochs
  early_stop_patience: 200 # Same as number of epochs by default, so no early stopping used
  accumulate_batches: 1
  gradient_clip: 0. # [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] 0 == no gradient clipping
  median_window:  [6, 20, 7, 4, 7, 6, 6, 6, 6, 6] # default: [7, 7, 7, 7, 7, 7, 7, 7, 7, 7] , [6, 20, 7, 4, 7, 6, 6, 6, 6, 6]
  val_thresholds: [0.5] # [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]
  n_test_thresholds: 50 # [50, 100, 200, 300, 400, 500]
  ema_factor: 0.999 # ['0.999', '0.99', '0.9', '0.8', '0.7', '0.6', '0.5', '0.4', '0.3', '0.2', '0.1']
  self_sup_loss: mse # ['bce', 'mse'] for self supervised mean teacher loss
  backend: auto # ['ddp', 'dp', 'None']
  validation_interval: 10
  weak_split: 0.9
  seed: 42
  deterministic: False  # deterministic must be false since we are using adaptive-pool
  precision: 32
  mixup: soft # ['soft', 'hard']
  obj_metric_synth_type: intersection
  enable_progress_bar: True
  use_unlabeled: False
scaler:
  statistic: instance # ['instance', 'dataset']
  normtype: minmax # ['minmax', 'standard', 'mean']
  dims: [1, 2] # normalization dims
  savepath: ./scaler.ckpt
data:
  synth_folder: "data/DESED/audio/strong_synth_16k/train/soundscapes_16k/"
  synth_tsv:  "data/DESED/annotations/synth_train(10000).tsv"

  synth_val_folder: "data/DESED/audio/strong_synth_16k/val/soundscapes_16k/"
  synth_val_tsv:  "data/DESED/annotations/synth_val(2500).tsv"
  synth_val_dur: "data/DESED/annotations/synth_val_duration.tsv"

  strong_folder: "data/DESED/audio/strong_real_16k/"
  strong_val_tsv: "data/DESED/annotations/real_validation(1168).tsv"
  strong_val_dur: "data/DESED/annotations/real_validation_durations.tsv"
  strong_tsv: "data/DESED/annotations/real_audioset_strong(3373).tsv"

  weak_folder: "data/DESED/audio/weak_16k/"
  weak_tsv: "data/DESED/annotations/weak(1578).tsv"

  # unlabeled_folder: "data/DESED/audio/weak_16k/"

  # ===== Test set 1: Public eval set 2019 =====
  test_tsv: "data/DESED/annotations/public.tsv"
  test_folder: "data/DESED/audio/eval/public/"
  test_dur: "data/DESED/annotations/public_dur.tsv"

  # ===== Test set 2: Public eval set 2024 =====
  # test_tsv: "data/DESED/annotations/public.tsv"
  # test_folder: "data/DESED/audio/eval/eval24/"
  # test_dur: "data/DESED/annotations/public_eval24_dur.tsv"

  # ===== Test set 3: data/DESED validation set =====
  # test_folder: "data/DESED/audio/validation_16k/"
  # test_tsv: "data/DESED/annotations/real_validation(1168).tsv"
  # test_dur: "data/DESED/annotations/real_validation_durations.tsv"
  # eval_folder: "data/DESED/audio/eval21_16k"

  external_folder: # External data path extracted mannually
  external_tsv: 
  external_weak_folder: 
  external_weak_tsv: 
  audio_max_len: 10
  fs: 16000
  net_subsample: 4
opt:
  mode: adam # adam or sgd or both
  cnn_lr: 0.0002
  rnn_lr: 0.002
  tfm_lr: 0.0002
  tfm_lr_scale: 0.5
  tfm_trainable_layers: 14 # max: 14; min: 0 for freezing
feats:
  n_mels: 128
  n_filters: 2048
  hop_length: 256
  n_window: 2048
  sample_rate: 16000
  f_min: 0
  f_max: 8000
net:
  dropout: 0.5
  rnn_layers: 2
  n_in_channel: 1
  nclass: 10
  attention: True
  n_RNN_cell: 128
  activation: cg
  rnn_type: BGRU
  kernel_size: [3, 3, 3, 3, 3, 3, 3]
  padding: [1, 1, 1, 1, 1, 1, 1]
  stride: [1, 1, 1, 1, 1, 1, 1]
  nb_filters: [ 16, 32, 64, 128, 128, 128, 128 ]
  pooling: [ [ 2, 2 ], [ 2, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ], [ 1, 2 ] ]
  dropout_recurrent: 0
  use_embeddings: True
  embedding_size: 768
  embedding_type: frame
  aggregation_type: pool1d
ultra:
  atst_dropout: 0.0
  model_init: data/stage_1.ckpt # path of model checkpoint from stage 1
  atst_init: data/atst_as2M.ckpt # absolute path to checkpoint
comments: 